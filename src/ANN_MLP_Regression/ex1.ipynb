{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a99b6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c483177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost or loss function\n",
    "def cost(Y, Yhat):\n",
    "    return np.mean((Yhat - Y) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72e677d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SAT</th>\n",
       "      <th>GPA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1714</td>\n",
       "      <td>2.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1664</td>\n",
       "      <td>2.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1760</td>\n",
       "      <td>2.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1685</td>\n",
       "      <td>2.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1693</td>\n",
       "      <td>2.83</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    SAT   GPA\n",
       "0  1714  2.40\n",
       "1  1664  2.52\n",
       "2  1760  2.54\n",
       "3  1685  2.74\n",
       "4  1693  2.83"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '../../data/KNN_Linear_Regression/SAT_GPA.csv'\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbcd6f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"SAT\"]\n",
    "y = df[\"GPA\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=30/84, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21605389",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.to_numpy().reshape(-1, 1)\n",
    "X_test = X_test.to_numpy().reshape(-1, 1)\n",
    "\n",
    "y_train = y_train.to_numpy().reshape(-1, 1)\n",
    "y_test = y_test.to_numpy().reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097880b5",
   "metadata": {},
   "source": [
    "**Huấn luyện mô hình trên tập Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "087c889d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 0.467664\n",
      "iter 1000, loss: 0.343570\n",
      "iter 2000, loss: 0.334343\n",
      "iter 3000, loss: 0.332861\n",
      "iter 4000, loss: 0.332679\n",
      "iter 5000, loss: 0.332593\n",
      "iter 6000, loss: 0.332596\n",
      "iter 7000, loss: 0.332574\n",
      "iter 8000, loss: 0.332681\n",
      "iter 9000, loss: 0.332575\n",
      "\n",
      "Training Time: 0.492591 s\n"
     ]
    }
   ],
   "source": [
    "d0 = 1\n",
    "d1 = h = 100 # size of hidden layer\n",
    "d2 = C = 1\n",
    "\n",
    "# initialize parameters randomly\n",
    "W1 = 0.01*np.random.randn(d0, d1)\n",
    "b1 = np.zeros((d1, 1))\n",
    "W2 = 0.01*np.random.randn(d1, d2)\n",
    "b2 = np.zeros((d2, 1))\n",
    "\n",
    "N = X_train.T.shape[1]\n",
    "eta = 0.0005 # learning rate\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(10000):\n",
    "    ## Feedforward\n",
    "    Z1 = np.dot(W1.T, X_train.T) + b1\n",
    "    A1 = np.where(Z1 > 0, Z1, 0.01 * Z1)  # LeakyReLU\n",
    "    Z2 = np.dot(W2.T, A1) + b2\n",
    "    Yhat = Z2\n",
    "\n",
    "    # print loss after each 1000 iterations\n",
    "    if i %1000 == 0:\n",
    "        # compute the loss: average cross-entropy loss\n",
    "        loss = cost(y_train, Yhat)\n",
    "        print(\"iter %d, loss: %f\" %(i, loss))\n",
    "\n",
    "    # backpropagation\n",
    "    E2 = (Yhat - y_train.T)/N\n",
    "    dW2 = np.dot(A1, E2.T)\n",
    "    db2 = np.sum(E2, axis = 1, keepdims = True)\n",
    "    E1 = np.dot(W2, E2)\n",
    "    E1 = np.where(Z1 > 0, E1, 0.01 * E1)  # Gradient of LeakyReLU\n",
    "    dW1 = np.dot(X_train.T, E1.T)\n",
    "    db1 = np.sum(E1, axis = 1, keepdims = True)\n",
    "    \n",
    "    # Gradient clipping (To avoid booming gradient)\n",
    "    clip_value = 1.0\n",
    "    dW1 = np.clip(dW1, -clip_value, clip_value)\n",
    "    dW2 = np.clip(dW2, -clip_value, clip_value)\n",
    "    db1 = np.clip(db1, -clip_value, clip_value)\n",
    "    db2 = np.clip(db2, -clip_value, clip_value)\n",
    "    \n",
    "    # Gradient Descent update\n",
    "    W1 += -eta*dW1\n",
    "    b1 += -eta*db1\n",
    "    W2 += -eta*dW2\n",
    "    b2 += -eta*db2\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\nTraining Time: %f s\" % training_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b643b6",
   "metadata": {},
   "source": [
    "**Dự đoán đầu ra trên tập Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e986e8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.00863641, 2.91275828, 2.69403629, 2.95620306, 2.90077351,\n",
       "        3.07455262, 2.75995251, 2.59666007, 2.91425637, 2.54123052,\n",
       "        2.79590681, 2.94272019, 2.74946584, 2.96219544, 2.8318611 ,\n",
       "        2.9741802 , 2.70152677, 2.52475147, 2.5846753 , 2.91725256,\n",
       "        2.68504772, 2.65958009, 2.7284925 , 2.95919925, 2.58167911,\n",
       "        2.50827241, 2.67605914, 2.66707057, 2.84384587, 2.98167068,\n",
       "        2.78841633, 2.85283445, 2.54422671, 2.55321529, 3.04459071,\n",
       "        2.67306295, 2.70452296, 3.04309261, 2.67605914, 2.77643156,\n",
       "        2.6550858 , 2.71650773, 2.75246203, 2.65958009, 2.68055343,\n",
       "        2.50977051, 2.64609723, 2.85283445, 2.51875908, 3.03560213,\n",
       "        2.78841633, 2.9756783 , 2.61613531, 2.94421829]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z1 = np.dot(W1.T, X_train.T) + b1\n",
    "A1 = np.maximum(Z1, 0)\n",
    "Z2 = np.dot(W2.T, A1) + b2\n",
    "\n",
    "Z2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a40b761",
   "metadata": {},
   "source": [
    "**Thời gian predict trung bình**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd85aecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Predict Time: 0.000000 s\n"
     ]
    }
   ],
   "source": [
    "num_runs = 10\n",
    "prediction_times = []\n",
    "\n",
    "for _ in range(num_runs):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    Z1 = np.dot(W1.T, X_train.T) + b1\n",
    "    A1 = np.maximum(Z1, 0)\n",
    "    Z2 = np.dot(W2.T, A1) + b2\n",
    "    \n",
    "    prediction_time = time.time() - start_time\n",
    "    prediction_times.append(prediction_time)\n",
    "\n",
    "avg_prediction_time = np.mean(prediction_times)\n",
    "print(\"Average Predict Time: %f s\" % avg_prediction_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7a4da8",
   "metadata": {},
   "source": [
    "**Đánh giá độ chính xác:**\n",
    "\n",
    "* R square = -2.948 < 0 => Mô hình không phù hợp \n",
    "\n",
    "* MSE = 0.313 là khá cao khi xét trong khoảng giá trị dự đoán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b75c000c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R square: -2.94834974793418\n",
      "MSE: 0.3136479132206353\n"
     ]
    }
   ],
   "source": [
    "print(\"R square:\", r2_score(y_train, Z2[0]))\n",
    "print(\"MSE:\", mean_squared_error(y_train, Z2[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e2734e",
   "metadata": {},
   "source": [
    "**Huấn luyện mô hình trên tập Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c6e1b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 9.589682\n",
      "iter 1000, loss: 0.153308\n",
      "iter 2000, loss: 0.150617\n",
      "iter 3000, loss: 0.142201\n",
      "iter 4000, loss: 0.141465\n",
      "iter 5000, loss: 0.141406\n",
      "iter 6000, loss: 0.141329\n",
      "iter 7000, loss: 0.141333\n",
      "iter 8000, loss: 0.141359\n",
      "iter 9000, loss: 0.141325\n",
      "\n",
      "Training Time: 0.450134 s\n"
     ]
    }
   ],
   "source": [
    "d0 = 1\n",
    "d1 = h = 100 # size of hidden layer\n",
    "d2 = C = 1\n",
    "# initialize parameters randomly\n",
    "W1 = 0.01*np.random.randn(d0, d1)\n",
    "b1 = np.zeros((d1, 1))\n",
    "W2 = 0.01*np.random.randn(d1, d2)\n",
    "b2 = np.zeros((d2, 1))\n",
    "\n",
    "N = X_test.T.shape[1]\n",
    "eta = 0.0005 # learning rate\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(10000):\n",
    "    ## Feedforward\n",
    "    Z1 = np.dot(W1.T, X_test.T) + b1\n",
    "    A1 = np.where(Z1 > 0, Z1, 0.01 * Z1)  # LeakyReLU\n",
    "    Z2 = np.dot(W2.T, A1) + b2\n",
    "    Yhat = Z2\n",
    "\n",
    "    # print loss after each 1000 iterations\n",
    "    if i %1000 == 0:\n",
    "        # compute the loss: average cross-entropy loss\n",
    "        loss = cost(y_test, Yhat)\n",
    "        print(\"iter %d, loss: %f\" %(i, loss))\n",
    "\n",
    "    # backpropagation\n",
    "    E2 = (Yhat - y_test.T)/N\n",
    "    dW2 = np.dot(A1, E2.T)\n",
    "    db2 = np.sum(E2, axis = 1, keepdims = True)\n",
    "    E1 = np.dot(W2, E2)\n",
    "    E1 = np.where(Z1 > 0, E1, 0.01 * E1)  # Gradient of LeakyReLU\n",
    "    dW1 = np.dot(X_test.T, E1.T)\n",
    "    db1 = np.sum(E1, axis = 1, keepdims = True)\n",
    "    \n",
    "    # Gradient clipping\n",
    "    clip_value = 1.0\n",
    "    dW1 = np.clip(dW1, -clip_value, clip_value)\n",
    "    dW2 = np.clip(dW2, -clip_value, clip_value)\n",
    "    db1 = np.clip(db1, -clip_value, clip_value)\n",
    "    db2 = np.clip(db2, -clip_value, clip_value)\n",
    "    \n",
    "    # Gradient Descent update\n",
    "    W1 += -eta*dW1\n",
    "    b1 += -eta*db1\n",
    "    W2 += -eta*dW2\n",
    "    b2 += -eta*db2\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\nTraining Time: %f s\" % training_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f8a456",
   "metadata": {},
   "source": [
    "**Dự đoán đầu ra trên tập Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "601d6f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.46875201, 3.8677027 , 3.78149728, 3.70932529, 4.10025688,\n",
       "        4.04011356, 3.75944472, 3.55295266, 3.9118078 , 3.45872812,\n",
       "        3.68526796, 3.72135395, 3.61510075, 3.6993014 , 3.26626949,\n",
       "        3.66321541, 3.8677027 , 3.61510075, 3.64116286, 3.40259435,\n",
       "        3.72937306, 3.51887144, 3.9739559 , 3.69729663, 3.71333484,\n",
       "        3.74340651, 3.86168837, 3.81357371, 3.57099565, 3.97997023]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z1 = np.dot(W1.T, X_test.T) + b1\n",
    "A1 = np.maximum(Z1, 0)\n",
    "Z2 = np.dot(W2.T, A1) + b2\n",
    "\n",
    "Z2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799158f7",
   "metadata": {},
   "source": [
    "**Thời gian predict trung bình**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bb1a666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Predict Time: 0.000000 s\n"
     ]
    }
   ],
   "source": [
    "num_runs = 10\n",
    "prediction_times = []\n",
    "\n",
    "for _ in range(num_runs):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    Z1 = np.dot(W1.T, X_test.T) + b1\n",
    "    A1 = np.maximum(Z1, 0)\n",
    "    Z2 = np.dot(W2.T, A1) + b2\n",
    "    \n",
    "    prediction_time = time.time() - start_time\n",
    "    prediction_times.append(prediction_time)\n",
    "\n",
    "avg_prediction_time = np.mean(prediction_times)\n",
    "print(\"Average Predict Time: %f s\" % avg_prediction_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af3477a",
   "metadata": {},
   "source": [
    "**Đánh giá độ chính xác:**\n",
    "\n",
    "* R square = -1.746 < 0 => Mô hình không phù hợp \n",
    "\n",
    "* MSE = 0.148 là hơi cao khi xét trong khoảng giá trị dự đoán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fef0aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R square: -1.7469342194889128\n",
      "MSE: 0.14828683432593015\n"
     ]
    }
   ],
   "source": [
    "print(\"R square:\", r2_score(y_test, Z2[0]))\n",
    "print(\"MSE:\", mean_squared_error(y_test, Z2[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e394c4b8",
   "metadata": {},
   "source": [
    "**Sử dụng mô hình hồi quy tuyến tính**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d58fe39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time: 0.002434 s\n",
      "Average Predict Time: 0.000037 s\n",
      "R square: 0.05950572997060655\n",
      "MSE: 0.05077038868090673\n"
     ]
    }
   ],
   "source": [
    "linR = LinearRegression()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "linR.fit(X_train, y_train)\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(\"Training Time: %f s\" % training_time)\n",
    "\n",
    "\n",
    "num_runs = 10\n",
    "prediction_times = []\n",
    "\n",
    "for _ in range(num_runs):\n",
    "    start_time = time.time()\n",
    "    y_pred = linR.predict(X_test)\n",
    "    prediction_time = time.time() - start_time\n",
    "    prediction_times.append(prediction_time)\n",
    "    \n",
    "avg_prediction_time = np.mean(prediction_times)\n",
    "print(\"Average Predict Time: %f s\" % avg_prediction_time)\n",
    "\n",
    "print(\"R square:\", r2_score(y_test, y_pred))\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82ae06d",
   "metadata": {},
   "source": [
    "**So sánh với mô hình ANN:**\n",
    "\n",
    "* Thời gian training: Mô hình `Linear Regression` nhanh hơn nhiều so với mô hình `ANN`\n",
    "\n",
    "* Thời gian predict: Hai mô hình tương đương nhau\n",
    "\n",
    "* R square: Mô hình `Linear Regression` có R square = 0.059 nằm trong khoảng (0, 1), cho thấy các biến độc lập giải thích được khoảng 5.9% sự biến thiên của biến phụ thuộc. nhưng như thế vẫn là tốt hơn so với `ANN` khi R square < 0 \n",
    "\n",
    "* MSE: Sai số trung bình của mô hình `Linear Regression` là 0.051, là nhỏ hơn hẳn so với mô hình `ANN`, cho thấy `Linear Regression` dự đoán chính xác hơn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732d5ec9",
   "metadata": {},
   "source": [
    "**Mô hình `ANN` với số chiều layer ẩn là 75**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "759942f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 21.215327\n",
      "iter 1000, loss: 0.171394\n",
      "iter 2000, loss: 0.170215\n",
      "iter 3000, loss: 0.178774\n",
      "iter 4000, loss: 0.180008\n",
      "iter 5000, loss: 0.180206\n",
      "iter 6000, loss: 0.180227\n",
      "iter 7000, loss: 0.180207\n",
      "iter 8000, loss: 0.180231\n",
      "iter 9000, loss: 0.180235\n",
      "\n",
      "Training Time: 0.458576 s\n",
      "R square: -1.2751496073768074\n",
      "MSE: 0.12281864287181972\n"
     ]
    }
   ],
   "source": [
    "d0 = 1\n",
    "d1 = h = 75 # size of hidden layer\n",
    "d2 = C = 1\n",
    "# initialize parameters randomly\n",
    "W1 = 0.01*np.random.randn(d0, d1)\n",
    "b1 = np.zeros((d1, 1))\n",
    "W2 = 0.01*np.random.randn(d1, d2)\n",
    "b2 = np.zeros((d2, 1))\n",
    "\n",
    "N = X_train.T.shape[1]\n",
    "eta = 0.0005 # learning rate\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(10000):\n",
    "    ## Feedforward\n",
    "    Z1 = np.dot(W1.T, X_train.T) + b1\n",
    "    A1 = np.where(Z1 > 0, Z1, 0.01 * Z1)  # LeakyReLU\n",
    "    Z2 = np.dot(W2.T, A1) + b2\n",
    "    Yhat = Z2\n",
    "\n",
    "    # print loss after each 1000 iterations\n",
    "    if i %1000 == 0:\n",
    "        # compute the loss: average cross-entropy loss\n",
    "        loss = cost(y_train, Yhat)\n",
    "        print(\"iter %d, loss: %f\" %(i, loss))\n",
    "\n",
    "    # backpropagation\n",
    "    E2 = (Yhat - y_train.T)/N\n",
    "    dW2 = np.dot(A1, E2.T)\n",
    "    db2 = np.sum(E2, axis = 1, keepdims = True)\n",
    "    E1 = np.dot(W2, E2)\n",
    "    E1 = np.where(Z1 > 0, E1, 0.01 * E1)  # Gradient of LeakyReLU\n",
    "    dW1 = np.dot(X_train.T, E1.T)\n",
    "    db1 = np.sum(E1, axis = 1, keepdims = True)\n",
    "    \n",
    "    # Gradient clipping\n",
    "    clip_value = 1.0\n",
    "    dW1 = np.clip(dW1, -clip_value, clip_value)\n",
    "    dW2 = np.clip(dW2, -clip_value, clip_value)\n",
    "    db1 = np.clip(db1, -clip_value, clip_value)\n",
    "    db2 = np.clip(db2, -clip_value, clip_value)\n",
    "    \n",
    "    # Gradient Descent update\n",
    "    W1 += -eta*dW1\n",
    "    b1 += -eta*db1\n",
    "    W2 += -eta*dW2\n",
    "    b2 += -eta*db2\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\nTraining Time: %f s\" % training_time)\n",
    "\n",
    "\n",
    "Z1 = np.dot(W1.T, X_test.T) + b1\n",
    "A1 = np.maximum(Z1, 0)\n",
    "Z2 = np.dot(W2.T, A1) + b2\n",
    "\n",
    "print(\"R square:\", r2_score(y_test, Z2[0]))\n",
    "print(\"MSE:\", mean_squared_error(y_test, Z2[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8658345",
   "metadata": {},
   "source": [
    "Khi giảm số chiều layer ẩn xuống 75.\n",
    "\n",
    "Ta thấy `R square` của mô hình tăng lên và `MSE` giảm xuống.\n",
    "\n",
    "=> Mô hình trở nên tốt hơn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aa3df0",
   "metadata": {},
   "source": [
    "**Mô hình `ANN` với số chiều layer ẩn là 50**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd91d900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 11.499286\n",
      "iter 1000, loss: 0.147084\n",
      "iter 2000, loss: 0.145789\n",
      "iter 3000, loss: 0.143994\n",
      "iter 4000, loss: 0.142153\n",
      "iter 5000, loss: 0.139940\n",
      "iter 6000, loss: 0.137201\n",
      "iter 7000, loss: 0.136895\n",
      "iter 8000, loss: 0.135084\n",
      "iter 9000, loss: 0.135063\n",
      "\n",
      "Training Time: 0.399914 s\n",
      "R square: -3.6264598199223403\n",
      "MSE: 0.24974863830559435\n"
     ]
    }
   ],
   "source": [
    "d0 = 1\n",
    "d1 = h = 50 # size of hidden layer\n",
    "d2 = C = 1\n",
    "# initialize parameters randomly\n",
    "W1 = 0.01*np.random.randn(d0, d1)\n",
    "b1 = np.zeros((d1, 1))\n",
    "W2 = 0.01*np.random.randn(d1, d2)\n",
    "b2 = np.zeros((d2, 1))\n",
    "\n",
    "N = X_train.T.shape[1]\n",
    "eta = 0.0005 # learning rate\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(10000):\n",
    "    ## Feedforward\n",
    "    Z1 = np.dot(W1.T, X_train.T) + b1\n",
    "    A1 = np.where(Z1 > 0, Z1, 0.01 * Z1)  # LeakyReLU\n",
    "    Z2 = np.dot(W2.T, A1) + b2\n",
    "    Yhat = Z2\n",
    "\n",
    "    # print loss after each 1000 iterations\n",
    "    if i %1000 == 0:\n",
    "        # compute the loss: average cross-entropy loss\n",
    "        loss = cost(y_train, Yhat)\n",
    "        print(\"iter %d, loss: %f\" %(i, loss))\n",
    "\n",
    "    # backpropagation\n",
    "    E2 = (Yhat - y_train.T)/N\n",
    "    dW2 = np.dot(A1, E2.T)\n",
    "    db2 = np.sum(E2, axis = 1, keepdims = True)\n",
    "    E1 = np.dot(W2, E2)\n",
    "    E1 = np.where(Z1 > 0, E1, 0.01 * E1)  # Gradient of LeakyReLU\n",
    "    dW1 = np.dot(X_train.T, E1.T)\n",
    "    db1 = np.sum(E1, axis = 1, keepdims = True)\n",
    "    \n",
    "    # Gradient clipping\n",
    "    clip_value = 1.0\n",
    "    dW1 = np.clip(dW1, -clip_value, clip_value)\n",
    "    dW2 = np.clip(dW2, -clip_value, clip_value)\n",
    "    db1 = np.clip(db1, -clip_value, clip_value)\n",
    "    db2 = np.clip(db2, -clip_value, clip_value)\n",
    "    \n",
    "    # Gradient Descent update\n",
    "    W1 += -eta*dW1\n",
    "    b1 += -eta*db1\n",
    "    W2 += -eta*dW2\n",
    "    b2 += -eta*db2\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\nTraining Time: %f s\" % training_time)\n",
    "\n",
    "Z1 = np.dot(W1.T, X_test.T) + b1\n",
    "A1 = np.maximum(Z1, 0)\n",
    "Z2 = np.dot(W2.T, A1) + b2\n",
    "\n",
    "print(\"R square:\", r2_score(y_test, Z2[0]))\n",
    "print(\"MSE:\", mean_squared_error(y_test, Z2[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701b124a",
   "metadata": {},
   "source": [
    "Khi giảm số chiều xuống 50.\n",
    "\n",
    "Ta thấy `R square` lại giảm đi rất nhiều và `MSE` lại tăng lên.\n",
    "\n",
    "=> Đã giảm số chiều layer ẩn quá nhiều khiến mô hình không học được đặc trưng và dẫn đến underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740c2d3f",
   "metadata": {},
   "source": [
    "**Nhận xét mối liên hệ giữa siêu tham số số chiều layer ẩn:**\n",
    "\n",
    "* Số chiều layer ẩn quyết định dung lượng mô hình. \n",
    "\n",
    "* Số chiều lớn tăng khả năng học nhưng dễ overfitting; Số chiều nhỏ giảm nguy cơ overfitting nhưng lại tăng nguy cơ underfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
