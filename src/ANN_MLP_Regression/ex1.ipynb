{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a99b6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c483177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost or loss function\n",
    "def cost(Y, Yhat):\n",
    "    return np.mean((Yhat - Y) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72e677d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SAT</th>\n",
       "      <th>GPA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1714</td>\n",
       "      <td>2.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1664</td>\n",
       "      <td>2.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1760</td>\n",
       "      <td>2.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1685</td>\n",
       "      <td>2.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1693</td>\n",
       "      <td>2.83</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    SAT   GPA\n",
       "0  1714  2.40\n",
       "1  1664  2.52\n",
       "2  1760  2.54\n",
       "3  1685  2.74\n",
       "4  1693  2.83"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '../../data/KNN_Linear_Regression/SAT_GPA.csv'\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbcd6f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"SAT\"]\n",
    "y = df[\"GPA\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=30/84, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21605389",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.to_numpy().reshape(-1, 1)\n",
    "X_test = X_test.to_numpy().reshape(-1, 1)\n",
    "\n",
    "y_train = y_train.to_numpy().reshape(-1, 1)\n",
    "y_test = y_test.to_numpy().reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "087c889d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 4.356787\n",
      "iter 1000, loss: 0.257125\n",
      "iter 2000, loss: 0.236572\n",
      "iter 3000, loss: 0.224616\n",
      "iter 4000, loss: 0.213480\n",
      "iter 5000, loss: 0.213319\n",
      "iter 6000, loss: 0.213300\n",
      "iter 7000, loss: 0.213351\n",
      "iter 8000, loss: 0.213305\n",
      "iter 9000, loss: 0.213324\n",
      "\n",
      "Training Time: 0.512405 s\n"
     ]
    }
   ],
   "source": [
    "d0 = 1\n",
    "d1 = h = 100 # size of hidden layer\n",
    "d2 = C = 1\n",
    "# initialize parameters randomly\n",
    "W1 = 0.01*np.random.randn(d0, d1)\n",
    "b1 = np.zeros((d1, 1))\n",
    "W2 = 0.01*np.random.randn(d1, d2)\n",
    "b2 = np.zeros((d2, 1))\n",
    "\n",
    "N = X_train.T.shape[1]\n",
    "eta = 0.0005 # learning rate\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(10000):\n",
    "    ## Feedforward\n",
    "    Z1 = np.dot(W1.T, X_train.T) + b1\n",
    "    A1 = np.where(Z1 > 0, Z1, 0.01 * Z1)  # LeakyReLU\n",
    "    Z2 = np.dot(W2.T, A1) + b2\n",
    "    Yhat = Z2\n",
    "\n",
    "    # print loss after each 1000 iterations\n",
    "    if i %1000 == 0:\n",
    "        # compute the loss: average cross-entropy loss\n",
    "        loss = cost(y_train, Yhat)\n",
    "        print(\"iter %d, loss: %f\" %(i, loss))\n",
    "\n",
    "    # backpropagation\n",
    "    E2 = (Yhat - y_train.T)/N\n",
    "    dW2 = np.dot(A1, E2.T)\n",
    "    db2 = np.sum(E2, axis = 1, keepdims = True)\n",
    "    E1 = np.dot(W2, E2)\n",
    "    E1 = np.where(Z1 > 0, E1, 0.01 * E1)  # Gradient of LeakyReLU\n",
    "    dW1 = np.dot(X_train.T, E1.T)\n",
    "    db1 = np.sum(E1, axis = 1, keepdims = True)\n",
    "    \n",
    "    # Gradient clipping\n",
    "    clip_value = 1.0\n",
    "    dW1 = np.clip(dW1, -clip_value, clip_value)\n",
    "    dW2 = np.clip(dW2, -clip_value, clip_value)\n",
    "    db1 = np.clip(db1, -clip_value, clip_value)\n",
    "    db2 = np.clip(db2, -clip_value, clip_value)\n",
    "    \n",
    "    # Gradient Descent update\n",
    "    W1 += -eta*dW1\n",
    "    b1 += -eta*db1\n",
    "    W2 += -eta*dW2\n",
    "    b2 += -eta*db2\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\nTraining Time: %f s\" % training_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e986e8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.1165482 , 3.98285102, 3.67785434, 4.04343255, 3.96613887,\n",
       "        4.20846501, 3.76977115, 3.54206814, 3.98494004, 3.46477447,\n",
       "        3.81990759, 4.02463139, 3.75514802, 4.05178863, 3.87004403,\n",
       "        4.06850077, 3.68829943, 3.44179526, 3.525356  , 3.98911808,\n",
       "        3.66532023, 3.62980692, 3.72590176, 4.04761059, 3.52117796,\n",
       "        3.41881606, 3.65278612, 3.64025201, 3.88675618, 4.07894587,\n",
       "        3.8094625 , 3.89929029, 3.4689525 , 3.48148661, 4.16668464,\n",
       "        3.64860808, 3.69247747, 4.16459562, 3.65278612, 3.79275035,\n",
       "        3.62353986, 3.70918961, 3.75932606, 3.62980692, 3.65905317,\n",
       "        3.42090508, 3.61100575, 3.89929029, 3.43343919, 4.15415053,\n",
       "        3.8094625 , 4.07058979, 3.56922538, 4.02672041]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z1 = np.dot(W1.T, X_train.T) + b1\n",
    "A1 = np.maximum(Z1, 0)\n",
    "Z2 = np.dot(W2.T, A1) + b2\n",
    "\n",
    "Z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd85aecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Predict Time: 0.000100 s\n"
     ]
    }
   ],
   "source": [
    "num_runs = 10\n",
    "prediction_times = []\n",
    "\n",
    "for _ in range(num_runs):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    Z1 = np.dot(W1.T, X_train.T) + b1\n",
    "    A1 = np.maximum(Z1, 0)\n",
    "    Z2 = np.dot(W2.T, A1) + b2\n",
    "    \n",
    "    prediction_time = time.time() - start_time\n",
    "    prediction_times.append(prediction_time)\n",
    "\n",
    "avg_prediction_time = np.mean(prediction_times)\n",
    "print(\"Average Predict Time: %f s\" % avg_prediction_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b75c000c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R square: -2.5791586907433435\n",
      "MSE: 0.2843202163193636\n"
     ]
    }
   ],
   "source": [
    "print(\"R square:\", r2_score(y_train, Z2[0]))\n",
    "print(\"MSE:\", mean_squared_error(y_train, Z2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c6e1b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 17.853502\n",
      "iter 1000, loss: 0.135222\n",
      "iter 2000, loss: 0.138426\n",
      "iter 3000, loss: 0.135594\n",
      "iter 4000, loss: 0.138681\n",
      "iter 5000, loss: 0.138793\n",
      "iter 6000, loss: 0.138769\n",
      "iter 7000, loss: 0.138743\n",
      "iter 8000, loss: 0.138717\n",
      "iter 9000, loss: 0.138692\n",
      "\n",
      "Training Time: 0.427711 s\n"
     ]
    }
   ],
   "source": [
    "d0 = 1\n",
    "d1 = h = 100 # size of hidden layer\n",
    "d2 = C = 1\n",
    "# initialize parameters randomly\n",
    "W1 = 0.01*np.random.randn(d0, d1)\n",
    "b1 = np.zeros((d1, 1))\n",
    "W2 = 0.01*np.random.randn(d1, d2)\n",
    "b2 = np.zeros((d2, 1))\n",
    "\n",
    "N = X_test.T.shape[1]\n",
    "eta = 0.0005 # learning rate\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(10000):\n",
    "    ## Feedforward\n",
    "    Z1 = np.dot(W1.T, X_test.T) + b1\n",
    "    A1 = np.where(Z1 > 0, Z1, 0.01 * Z1)  # LeakyReLU\n",
    "    Z2 = np.dot(W2.T, A1) + b2\n",
    "    Yhat = Z2\n",
    "\n",
    "    # print loss after each 1000 iterations\n",
    "    if i %1000 == 0:\n",
    "        # compute the loss: average cross-entropy loss\n",
    "        loss = cost(y_test, Yhat)\n",
    "        print(\"iter %d, loss: %f\" %(i, loss))\n",
    "\n",
    "    # backpropagation\n",
    "    E2 = (Yhat - y_test.T)/N\n",
    "    dW2 = np.dot(A1, E2.T)\n",
    "    db2 = np.sum(E2, axis = 1, keepdims = True)\n",
    "    E1 = np.dot(W2, E2)\n",
    "    E1 = np.where(Z1 > 0, E1, 0.01 * E1)  # Gradient of LeakyReLU\n",
    "    dW1 = np.dot(X_test.T, E1.T)\n",
    "    db1 = np.sum(E1, axis = 1, keepdims = True)\n",
    "    \n",
    "    # Gradient clipping\n",
    "    clip_value = 1.0\n",
    "    dW1 = np.clip(dW1, -clip_value, clip_value)\n",
    "    dW2 = np.clip(dW2, -clip_value, clip_value)\n",
    "    db1 = np.clip(db1, -clip_value, clip_value)\n",
    "    db2 = np.clip(db2, -clip_value, clip_value)\n",
    "    \n",
    "    # Gradient Descent update\n",
    "    W1 += -eta*dW1\n",
    "    b1 += -eta*db1\n",
    "    W2 += -eta*dW2\n",
    "    b2 += -eta*db2\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\nTraining Time: %f s\" % training_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "601d6f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.19381602, 3.55500691, 3.47696064, 3.41161957, 3.76555035,\n",
       "        3.71109946, 3.45699531, 3.27004726, 3.59493756, 3.18474087,\n",
       "        3.38983922, 3.42250975, 3.32631318, 3.40254442, 3.01049803,\n",
       "        3.36987389, 3.55500691, 3.32631318, 3.34990857, 3.13392004,\n",
       "        3.42976987, 3.23919176, 3.65120348, 3.4007294 , 3.41524963,\n",
       "        3.44247508, 3.54956182, 3.50600111, 3.28638253, 3.65664857]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z1 = np.dot(W1.T, X_test.T) + b1\n",
    "A1 = np.maximum(Z1, 0)\n",
    "Z2 = np.dot(W2.T, A1) + b2\n",
    "\n",
    "Z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bb1a666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Predict Time: 0.000000 s\n"
     ]
    }
   ],
   "source": [
    "num_runs = 10\n",
    "prediction_times = []\n",
    "\n",
    "for _ in range(num_runs):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    Z1 = np.dot(W1.T, X_test.T) + b1\n",
    "    A1 = np.maximum(Z1, 0)\n",
    "    Z2 = np.dot(W2.T, A1) + b2\n",
    "    \n",
    "    prediction_time = time.time() - start_time\n",
    "    prediction_times.append(prediction_time)\n",
    "\n",
    "avg_prediction_time = np.mean(prediction_times)\n",
    "print(\"Average Predict Time: %f s\" % avg_prediction_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fef0aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R square: 0.1464407477745484\n",
      "MSE: 0.04607740459313581\n"
     ]
    }
   ],
   "source": [
    "print(\"R square:\", r2_score(y_test, Z2[0]))\n",
    "print(\"MSE:\", mean_squared_error(y_test, Z2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d58fe39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time: 0.002999 s\n",
      "Average Predict Time: 0.000102 s\n",
      "R square: 0.05950572997060655\n",
      "MSE: 0.05077038868090673\n"
     ]
    }
   ],
   "source": [
    "linR = LinearRegression()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "linR.fit(X_train, y_train)\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(\"Training Time: %f s\" % training_time)\n",
    "\n",
    "\n",
    "num_runs = 10\n",
    "prediction_times = []\n",
    "\n",
    "for _ in range(num_runs):\n",
    "    start_time = time.time()\n",
    "    y_pred = linR.predict(X_test)\n",
    "    prediction_time = time.time() - start_time\n",
    "    prediction_times.append(prediction_time)\n",
    "    \n",
    "avg_prediction_time = np.mean(prediction_times)\n",
    "print(\"Average Predict Time: %f s\" % avg_prediction_time)\n",
    "\n",
    "print(\"R square:\", r2_score(y_test, y_pred))\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "759942f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 15.533006\n",
      "iter 1000, loss: 0.160218\n",
      "iter 2000, loss: 0.159284\n",
      "iter 3000, loss: 0.157566\n",
      "iter 4000, loss: 0.155446\n",
      "iter 5000, loss: 0.158287\n",
      "iter 6000, loss: 0.156989\n",
      "iter 7000, loss: 0.157734\n",
      "iter 8000, loss: 0.158166\n",
      "iter 9000, loss: 0.158173\n",
      "\n",
      "Training Time: 0.569203 s\n",
      "R square: 0.017371492386684118\n",
      "MSE: 0.053044907183653754\n"
     ]
    }
   ],
   "source": [
    "d0 = 1\n",
    "d1 = h = 75 # size of hidden layer\n",
    "d2 = C = 1\n",
    "# initialize parameters randomly\n",
    "W1 = 0.01*np.random.randn(d0, d1)\n",
    "b1 = np.zeros((d1, 1))\n",
    "W2 = 0.01*np.random.randn(d1, d2)\n",
    "b2 = np.zeros((d2, 1))\n",
    "\n",
    "N = X_train.T.shape[1]\n",
    "eta = 0.0005 # learning rate\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(10000):\n",
    "    ## Feedforward\n",
    "    Z1 = np.dot(W1.T, X_train.T) + b1\n",
    "    A1 = np.where(Z1 > 0, Z1, 0.01 * Z1)  # LeakyReLU\n",
    "    Z2 = np.dot(W2.T, A1) + b2\n",
    "    Yhat = Z2\n",
    "\n",
    "    # print loss after each 1000 iterations\n",
    "    if i %1000 == 0:\n",
    "        # compute the loss: average cross-entropy loss\n",
    "        loss = cost(y_train, Yhat)\n",
    "        print(\"iter %d, loss: %f\" %(i, loss))\n",
    "\n",
    "    # backpropagation\n",
    "    E2 = (Yhat - y_train.T)/N\n",
    "    dW2 = np.dot(A1, E2.T)\n",
    "    db2 = np.sum(E2, axis = 1, keepdims = True)\n",
    "    E1 = np.dot(W2, E2)\n",
    "    E1 = np.where(Z1 > 0, E1, 0.01 * E1)  # Gradient of LeakyReLU\n",
    "    dW1 = np.dot(X_train.T, E1.T)\n",
    "    db1 = np.sum(E1, axis = 1, keepdims = True)\n",
    "    \n",
    "    # Gradient clipping\n",
    "    clip_value = 1.0\n",
    "    dW1 = np.clip(dW1, -clip_value, clip_value)\n",
    "    dW2 = np.clip(dW2, -clip_value, clip_value)\n",
    "    db1 = np.clip(db1, -clip_value, clip_value)\n",
    "    db2 = np.clip(db2, -clip_value, clip_value)\n",
    "    \n",
    "    # Gradient Descent update\n",
    "    W1 += -eta*dW1\n",
    "    b1 += -eta*db1\n",
    "    W2 += -eta*dW2\n",
    "    b2 += -eta*db2\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\nTraining Time: %f s\" % training_time)\n",
    "\n",
    "\n",
    "Z1 = np.dot(W1.T, X_test.T) + b1\n",
    "A1 = np.maximum(Z1, 0)\n",
    "Z2 = np.dot(W2.T, A1) + b2\n",
    "\n",
    "print(\"R square:\", r2_score(y_test, Z2[0]))\n",
    "print(\"MSE:\", mean_squared_error(y_test, Z2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd91d900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 5.748342\n",
      "iter 1000, loss: 0.148538\n",
      "iter 2000, loss: 0.140860\n",
      "iter 3000, loss: 0.140944\n",
      "iter 4000, loss: 0.142364\n",
      "iter 5000, loss: 0.143029\n",
      "iter 6000, loss: 0.143265\n",
      "iter 7000, loss: 0.143344\n",
      "iter 8000, loss: 0.143370\n",
      "iter 9000, loss: 0.143341\n",
      "\n",
      "Training Time: 0.467271 s\n",
      "R square: 0.04518765001488845\n",
      "MSE: 0.05154331681846294\n"
     ]
    }
   ],
   "source": [
    "d0 = 1\n",
    "d1 = h = 50 # size of hidden layer\n",
    "d2 = C = 1\n",
    "# initialize parameters randomly\n",
    "W1 = 0.01*np.random.randn(d0, d1)\n",
    "b1 = np.zeros((d1, 1))\n",
    "W2 = 0.01*np.random.randn(d1, d2)\n",
    "b2 = np.zeros((d2, 1))\n",
    "\n",
    "N = X_train.T.shape[1]\n",
    "eta = 0.0005 # learning rate\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(10000):\n",
    "    ## Feedforward\n",
    "    Z1 = np.dot(W1.T, X_train.T) + b1\n",
    "    A1 = np.where(Z1 > 0, Z1, 0.01 * Z1)  # LeakyReLU\n",
    "    Z2 = np.dot(W2.T, A1) + b2\n",
    "    Yhat = Z2\n",
    "\n",
    "    # print loss after each 1000 iterations\n",
    "    if i %1000 == 0:\n",
    "        # compute the loss: average cross-entropy loss\n",
    "        loss = cost(y_train, Yhat)\n",
    "        print(\"iter %d, loss: %f\" %(i, loss))\n",
    "\n",
    "    # backpropagation\n",
    "    E2 = (Yhat - y_train.T)/N\n",
    "    dW2 = np.dot(A1, E2.T)\n",
    "    db2 = np.sum(E2, axis = 1, keepdims = True)\n",
    "    E1 = np.dot(W2, E2)\n",
    "    E1 = np.where(Z1 > 0, E1, 0.01 * E1)  # Gradient of LeakyReLU\n",
    "    dW1 = np.dot(X_train.T, E1.T)\n",
    "    db1 = np.sum(E1, axis = 1, keepdims = True)\n",
    "    \n",
    "    # Gradient clipping\n",
    "    clip_value = 1.0\n",
    "    dW1 = np.clip(dW1, -clip_value, clip_value)\n",
    "    dW2 = np.clip(dW2, -clip_value, clip_value)\n",
    "    db1 = np.clip(db1, -clip_value, clip_value)\n",
    "    db2 = np.clip(db2, -clip_value, clip_value)\n",
    "    \n",
    "    # Gradient Descent update\n",
    "    W1 += -eta*dW1\n",
    "    b1 += -eta*db1\n",
    "    W2 += -eta*dW2\n",
    "    b2 += -eta*db2\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\nTraining Time: %f s\" % training_time)\n",
    "\n",
    "Z1 = np.dot(W1.T, X_test.T) + b1\n",
    "A1 = np.maximum(Z1, 0)\n",
    "Z2 = np.dot(W2.T, A1) + b2\n",
    "\n",
    "print(\"R square:\", r2_score(y_test, Z2[0]))\n",
    "print(\"MSE:\", mean_squared_error(y_test, Z2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11385cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
